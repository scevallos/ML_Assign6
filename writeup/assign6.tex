\documentclass[11pt]{article}

\usepackage{tikz}
\usepackage{url}
\usepackage{fullpage}
\usepackage{graphicx}

\title{CS158 - Assignment 6\\A Gradient Descent Into Madness\\\small{Due: Friday, October 14 by 6:00pm}}
\author{}
\date{}

\parindent=0in

\parskip 7.2pt 

\begin{document}
\maketitle

\vspace{-0.8in}

\begin{center}
\includegraphics[scale=1]{figures/sudoku.jpg}

\end{center}

For this assignment we're going to try out a few gradient descent variants (including SVM).  The amount of code you'll need to write for this will be pretty minimal. 

\section{High-level requirements}

\begin{itemize}

\item Implement the gradient descent algorithm

\item Your approach will need to support two loss functions: hinge loss and exponential loss

\item Your approach will need to support three regularization type: none, L1 and L2

\item You will provide a short writeup that includes:

\begin{itemize}

\item an argument that your implementation is correct.  This can be of the form of examining text snippets with justification and/or showing experimental results.

\item at least one experimental result

\end{itemize}

\end{itemize}

\section{Implementation Requirements}

To get you started I have included some starter code at:

{\footnotesize
\url{http://www.cs.pomona.edu/~dkauchak/classes/cs158/assignments/assign6/assign6-starter.tar.gz}
}

This is more or less the same starter code as assignment 5 except I have add the \texttt{KNNClassifier} (so that everything compiles) and I have included a starter class \texttt{GradientDescentClassifier}.  This file is a copy of the \texttt{PerceptronClassifier} file with the addition of a few constants.  This should serve as your starting point.

Implement a gradient descent classifier.  Your implementation must:

\begin{itemize}

\item[-] support a zero parameter constructor.  By default you should use exponential loss, no regularization, $\lambda = 0.1$ and $\eta=0.1$

\item[-] include a method \texttt{setLoss} that takes an int and selects the loss function to use (based on the constants)

\item[-] include a method \texttt{setRegularization} that takes an int and selects the regularization method to use (based on the constants)

\item[-] include a method \texttt{setLambda} that takes a double and sets the lambda to use

\item[-] include a method \texttt{setEta} that takes a double and sets the eta to use (Note, in practice we'd use some sort of schedule of etas, e.g. one for each iteration, however, to keep it simple we'll just have a constant learning rate.)

\item[-] support training and testing based on these parameters

\item[-] update the model weights (both $w_i$ and $b$) after \emph{every} example.  There are two common ways for implementing gradient descent:

\begin{enumerate}

\item For each iteration, iterate over all the examples and aggregate the sum of the adjustments and then adjust the weights based on this.  This results in adjusting each of the weights of the model once \emph{per iteration over the entire data}.

\item Or, for each iteration, for each example, calculate the adjustment to the weights and then immediately adjust the model weights before moving to the next example.
\end{enumerate}

Both are correct and are gradient descent approaches in that they will make steps towards lower gradient.  We're going to implement option 2 for this assignment.  In practice, it works better to update every example, particularly for larger data sets.  The trajectory is a bit noisier since it's only based on a single example, however, the advantage is that you take into account changes in the weights immediately into the model for future gradient calculations.

\end{itemize}

\section{Writeup}

In addition to your code, include a writeup (in some reasonable file format) that includes two sections: \textbf{Algorithm Correctness} and \textbf{Experimentation}.  Because of the small amount of coding required for this assignment, the writeup will be a non-trivial part of your grade, so make sure to devote sufficient time to this.

\subsection*{Algorithm Correctness}

One of the challenges with implementing machine learning algorithms, particularly iterative approaches like this, is determining if they are operating correctly.  Include a short ($\sim$1 page), concrete justification of why your implementation of the hinge loss with L2 regularization is correct.  This could include snippets of code, along with an explanation, and/or short experimental results, e.g. incremental output of a small problem.  \textit{You will be graded based on how convincing and thorough your argument is.}



\subsection*{Experimentation}

Run one experiment that highlights something interesting about the algorithm, include the resulting data, and include a few sentences of explanation/analysis.  For example, you could investigate the optimal $\lambda$/$\eta$ for one of the variants on one of the data sets, or you could investigate how the different loss/regularization approaches work (though pick some reasonable lambda).  Plan on spending around an hour playing around with this.  \textit{You will be graded based on the quality of your experimental setup and your presentation.}

\section{Hints/observations}

\begin{itemize}

\item Tuning the hyperparameters (i.e. $\lambda$ and $\eta$) is extremely important for gradient descent to get the algorithms to work well.

\item Think about how you're going to tune the parameters.  When you have multiple hyperparameters the space of possible parameters goes up drastically since you have to consider possible combinations of both.  There are many ways to do this (be creative), but as before, definitely do it programmatically.

\item Exponential loss is very hard to tune for as the accuracy tends to jump around a lot.  Start with the hinge loss first and then start at similar values for the exponential loss.

\item You can use whatever dataset you'd like, however, datasets with a large number of features (and, therefore, a larger number of weights and higher dimensionality), and particularly sparse datasets (like our wine dataset), are going to be harder/slower to train for iterative methods like gradient descent.

\item Don't forget that $b$ is also a parameter of the model and needs to be updated each time.



\end{itemize}


\section{Extra Credit}

For those who would like to experiment (and push themselves) a bit more  (and of course, get a bit of extra credit) you can try out some of these extra credit options.   If you try out these options, include an extra file called \texttt{extra.txt} that describes what extra credit you did.

\begin{itemize}

\item Allow for an $\eta$ schedule rather than just a single constant.  I'll let you be creative about the best way to do this, but please just create a new copy of your classifier code rather than trying to alter the existing code.

\item Add additional regularization or loss functions.  If you do this, add appropriate constants for selecting these.

\item Do additional experimentation and include it in the write-up.  The amount earned will be based on how much effort you put in and how creative the experiment is.

\end{itemize}

\section{When You're Done}

Make sure that your code compiles, that your files are named as specified and that you have followed the specifications exactly (i.e. method names, number of parameters, etc.).

Create a directory with your last name, followed by the assignment number, for example, for this assignment mine would be \texttt{kauchak6}.  If you worked with a partner, put both last names.

Inside this directory, create a \texttt{code} directory and copy all of your code into this directory, maintaining the package structure.

Finally, also include your \texttt{writeup} file.

\texttt{tar} then \texttt{gzip} this folder and submit that file on the submission page on the course web page.

\subsection*{Commenting and code style}

Your code should be commented appropriately (though you don't need to go overboard).    The most important things:

\begin{itemize}
\item Your name (or names) and the assignment number should be at the top of each file
\item Each class and method should have an appropriate JavaDoc
\item If anything is complicated, it should include some comments.
\end{itemize}

There are many possible ways to approach this problem, which makes code style and comments very important here so that I can understand what you did.  For this reason, you will lose points for poorly commented or poorly organized code.

\end{document}